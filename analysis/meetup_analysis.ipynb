{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# svg, png. no inline pdf.\n",
    "\n",
    "# mpl inches conversion:\n",
    "# 5in x 5in   => 279px x 279px\n",
    "# 10in x 10in => 558px x 558px\n",
    "\n",
    "# presentation for Markdown mixu:\n",
    "# width:\n",
    "#   content column width  = 760px = 13.62 in\n",
    "# height:\n",
    "#   max viewport height   = 857px = 15.36 in\n",
    "#   ideal viewport height = 260px = 4.66 in\n",
    "# \n",
    "# max width 760\n",
    "# 10 inches -> 760px width\n",
    "# 5 inches -> 279px width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import pandas\n",
    "import mpld3\n",
    "#mpld3.enable_notebook()\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "#import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.decomposition\n",
    "import sklearn.feature_extraction\n",
    "import sklearn.feature_extraction.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from pprint import pprint\n",
    "import cPickle as pickle\n",
    "import unicodecsv as csv\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import analysis_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.seterr(divide='raise', over='raise', under='raise', invalid='raise');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the cities. We'll filter out any groups that weren't 'active' during the period of study (2012-2014, inclusive; three years). Active groups are those that have had an event with at least two attendees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_city2groups = analysis_tools.get_city2groups('GB')\n",
    "\n",
    "db_all_groups = sum(all_city2groups.itervalues(), [])\n",
    "    # all groups loaded from DB, including those not assigned to cities\n",
    "print \"all gorups, inc without cities\", len(db_all_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"number of groups without city\", len(all_city2groups.get('<unknown>', []))\n",
    "if '<unknown>' in all_city2groups:\n",
    "    del all_city2groups['<unknown>']\n",
    "\n",
    "num_grps_before_filter = sum(len(groups) for groups in all_city2groups.itervalues())\n",
    "print \"number of groups before filtering:\", num_grps_before_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_active(grp):\n",
    "    for event in grp['events_in_window']:\n",
    "        if len(event['attendee_ids']) >= 2:\n",
    "            return True\n",
    "    return False\n",
    "        \n",
    "for city_ident in all_city2groups.iterkeys():\n",
    "    all_city2groups[city_ident] = filter(is_active, all_city2groups[city_ident])\n",
    "    \n",
    "num_grps_after_filter = sum(len(groups) for groups in all_city2groups.itervalues())\n",
    "print \"number of groups after filtering:\", num_grps_after_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many usable groups do we have per city?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "group_counts = collections.Counter({city: len(groups) for city, groups in all_city2groups.iteritems()})\n",
    "\n",
    "for indx, (k, v) in enumerate(group_counts.most_common()):\n",
    "    print \"%-10s%-10s%-30s(pop %d)\" % (indx+1, v, k, k.pop)\n",
    "print \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(group_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_num_groups = 5\n",
    "keep_cities = [city for city, count in group_counts.iteritems() if count >= min_num_groups]\n",
    "\n",
    "print \"%d cities with at least %s Tech groups:\" % (len(keep_cities), min_num_groups)\n",
    "print ', '.join(map(str, keep_cities))\n",
    "\n",
    "city2groups = {city: all_city2groups[city] for city in keep_cities}  # all cities for study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_study_grps = sum(len(groups) for groups in city2groups.itervalues())\n",
    "print \"groups crawled (before filtering)                  \", num_grps_before_filter\n",
    "print \"active groups (after filtering)                    \", num_grps_after_filter\n",
    "print \"active groups in active cities (focus of analysis) \", num_study_grps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build dataframe with descriptors of each city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# city name; longitude; latitude;\n",
    "# population (geodemographic); num users subscribed (unique); num users attended event (unique)\n",
    "# num events; num meetup groups\n",
    "\n",
    "def extract_active_users(groups):\n",
    "    # active = attended at least one event\n",
    "    uniqs = set()\n",
    "    for group in groups:\n",
    "        for event in group['events_in_window']:\n",
    "            uniqs.update(event['attendee_ids'])\n",
    "    return tuple(uniqs)\n",
    "def count_active_users(groups):\n",
    "    return len(extract_active_users(groups))\n",
    "\n",
    "def extract_subscribed_users(groups):\n",
    "    # subscribed = subscribed to at least one group\n",
    "    uniqs = set()\n",
    "    for group in groups:\n",
    "        uniqs.update(group['member_ids'])\n",
    "    return tuple(uniqs)\n",
    "def count_subscribed_users(groups):\n",
    "    return len(extract_subscribed_users(groups))\n",
    "\n",
    "rows = []\n",
    "for city, groups in city2groups.iteritems():\n",
    "    row = collections.OrderedDict()\n",
    "    pop = float(city.pop)\n",
    "    row['name'] = str(city)\n",
    "    row['fua_id'] = str(city.fua_id)\n",
    "    row['lon'] = city.lon\n",
    "    row['lat'] = city.lat\n",
    "    row['pop'] = pop\n",
    "    row['groups'] = float(len(groups))\n",
    "        # by way of previous filtering, these are 'active' gruops -- at least one event\n",
    "        # with two attendees\n",
    "    row['users_members'] = float(count_subscribed_users(groups))\n",
    "    row['users_attendees'] = float(count_active_users(groups))\n",
    "    # population-normed measures:\n",
    "    row['groups_per_100k'] = row['groups'] / (pop/10**5)\n",
    "    row['users_members_per_100k'] = row['users_members'] / (pop/10**5)\n",
    "    row['users_attendees_per_100k'] = row['users_attendees'] / (pop/10**5)\n",
    "    rows.append(row)\n",
    "\n",
    "df_cities = pandas.DataFrame(rows, columns=rows[0].keys())\n",
    "print df_cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other data for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# canonical ordering of cities:\n",
    "all_cities = tuple(sorted(city2groups.iterkeys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shortname(city_name):\n",
    "    if not isinstance(city_name, (str, unicode)):\n",
    "        city_name = unicode(city_name)\n",
    "    # return shortname for city. e .g.,\n",
    "    #     UK::Bristol metropolitan area  ->  Bristol\n",
    "    match = re.match('^.*:.*:([a-zA-Z]+)[ -/]?.*', city_name)\n",
    "    if match is None:\n",
    "        return city_name\n",
    "    else:\n",
    "        return match.group(1)\n",
    "print shortname('UK::London')\n",
    "print shortname('UK::Glasgow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_geometries_fua(bmap, show=True):\n",
    "    bmap.readshapefile('geometries/fua/fua_uk_wgs84', 'shp_fua', color=(0.5, 0.5, 0.5, 1.0),\n",
    "                       zorder=1, drawbounds=show, linewidth=0.3)\n",
    "\n",
    "def add_geometries_london(bmap, show=True):\n",
    "    bmap.readshapefile('geometries/london/london_boroughs_wgs84', 'shp_london', color=(0.5, 0.5, 0.5, 1.0),\n",
    "                       zorder=1, drawbounds=show, linewidth=0.3)\n",
    "\n",
    "def default_basemap(ax, draw_fua=True, draw_london=True, **kwargs):\n",
    "    # NB, even if draw_* is False, shape data will still be loaded on to\n",
    "    # the basemap\n",
    "    # llcrnrlon=-11.372070, llcrnrlat=48.507377, urcrnrlon=2.5, urcrnrlat=59.7\n",
    "    bmap = Basemap(\n",
    "                ax=ax,\n",
    "                rsphere=(6378137.00, 6356752.3142),\n",
    "                resolution='i', projection='merc',\n",
    "                lat_0=40., lon_0=-20., lat_ts=20.,\n",
    "                **kwargs)\n",
    "    \n",
    "    #basem.drawcoastlines(color=[0, 0, 0, 0.4])\n",
    "    #basem.fillcontinents()\n",
    "    #basem.drawlsmask(land_color=(1,1,1,1), ocean_color='#9AC2EA', lakes=False, resolution='f')\n",
    "    #basem.drawmapboundary(color='k', fill_color='g')\n",
    "    \n",
    "    add_geometries_fua(bmap, draw_fua)\n",
    "    add_geometries_london(bmap, draw_london)\n",
    "    \n",
    "    ax.set_axis_bgcolor('#9AC2EA')\n",
    "    bmap.fillcontinents(zorder=-1, color=(1.0,1,1,1))\n",
    "    return bmap\n",
    "\n",
    "def add_frame(ax, fc='none', ec='#888888', lw=5):\n",
    "    l, r = ax.get_xlim()\n",
    "    b, t = ax.get_ylim()\n",
    "    ax.add_artist(mpl.patches.Rectangle([l,b], r-l, t-b, fc='none', ec='#888888', lw=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geography of meetups and activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import palettable\n",
    "\n",
    "fig, ax = plt.subplots(1, 1,)  # figsize=[30, 6.45]\n",
    "\n",
    "basem = default_basemap(ax=ax, draw_fua=False, draw_london=False,\n",
    "                llcrnrlon=-11.372070, llcrnrlat=49.5, urcrnrlon=4.2, urcrnrlat=59.7)\n",
    "\n",
    "#\n",
    "# choropleth\n",
    "mags = df_cities['groups_per_100k'].values\n",
    "\n",
    "norm = mpl.colors.Normalize(vmin=0, vmax=max(mags))\n",
    "cmap = mpl.cm.ScalarMappable(norm=norm, cmap=palettable.colorbrewer.sequential.Reds_3.get_mpl_colormap())\n",
    "    #'winter_r' palettable.colorbrewer.sequential.YlOrRd_5.get_mpl_colormap() 'YrOrRd'\n",
    "\n",
    "cmap.set_array(mags)\n",
    "\n",
    "fua2shape = dict(zip([info['id_fua'] for info in basem.shp_fua_info], basem.shp_fua))\n",
    "for mag, (indx, row) in zip(mags, df_cities.iterrows()):\n",
    "    assert mag == row['groups_per_100k']\n",
    "    \n",
    "    #\n",
    "    fua_id = row['fua_id']\n",
    "    shp = fua2shape[fua_id]\n",
    "    poly = Polygon(np.array(shp), True, fc=cmap.to_rgba(mag), lw=0.1, ec='k')\n",
    "    ax.add_artist(poly)\n",
    "    \n",
    "    #\n",
    "    x, y = basem(row['lon'], row['lat'])\n",
    "    ax.text(x, y, shortname(row['name']))\n",
    "\n",
    "add_frame(ax)\n",
    "\n",
    "cbar = plt.colorbar(cmap, orientation='vertical', shrink=0.6)\n",
    "cbar.set_label('Groups per 100k People')\n",
    "cbar.set_clim(vmin=0)\n",
    "\n",
    "fig.set_size_inches(8.0, 7.0)\n",
    "fig.savefig('out/uk_choropleth.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Where do events happen? \n",
    "# Include all groups in the DB (inc. not assigned to city).\n",
    "# Ignore events w/o venues\n",
    "\n",
    "seq_lon = []\n",
    "seq_lat = []\n",
    "for group in db_all_groups:\n",
    "    for event in group['events_in_window']:\n",
    "        if 'venue' not in event:\n",
    "            continue\n",
    "        venue = event['venue']\n",
    "        seq_lon.append(venue['lon'])\n",
    "        seq_lat.append(venue['lat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes, mark_inset \n",
    "\n",
    "#\n",
    "# Basemap\n",
    "fig = plt.figure(figsize=[18, 12])\n",
    "#gs = mpl.gridspec.GridSpec(2, 3, wspace=0.02, hspace=0.01)\n",
    "#ax1 = plt.subplot(gs[:,:-1])\n",
    "#ax2 = plt.subplot(gs[0,-1])\n",
    "#ax3 = plt.subplot(gs[1,-1])\n",
    "gs = mpl.gridspec.GridSpec(1, 3, wspace=0.10, hspace=0.01, width_ratios=[5, 6, 10])\n",
    "ax1 = plt.subplot(gs[0,0])\n",
    "ax2 = plt.subplot(gs[0,1])\n",
    "ax3 = plt.subplot(gs[0,2])\n",
    "\n",
    "viewports = [\n",
    "    {'llcrnrlon': -11.372070, 'llcrnrlat': 48.507377, 'urcrnrlon': 2.5, 'urcrnrlat': 59.7},  # UK\n",
    "    {'llcrnrlon': -5.972070, 'llcrnrlat': 49.807377, 'urcrnrlon': 2.0, 'urcrnrlat': 54.7},  # eng&wal\n",
    "    {'llcrnrlon': -0.206744, 'llcrnrlat': 51.48, 'urcrnrlon': -0.01, 'urcrnrlat': 51.560},  # Central London\n",
    "    ]\n",
    "\n",
    "for ax, view, s, alpha in zip([ax1, ax2, ax3], viewports, [1.5, 1.5, 3.5], [0.2, 0.2, 0.5]):\n",
    "    basem = default_basemap(ax=ax, draw_fua=True, draw_london=True,**view)\n",
    "    x, y = basem(seq_lon, seq_lat)\n",
    "    ax.scatter(x, y, s=s, marker='o', linewidths=0, c='g', alpha=alpha)\n",
    "\n",
    "ax1.set_title('UK')\n",
    "ax2.set_title('England & Wales')\n",
    "ax3.set_title('Central London')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# population, number of groups, number of (unique) active usres (by event attendance)\n",
    "\n",
    "df_cities.plot(kind='scatter', x='users_members_per_100k', y='groups_per_100k', marker='x')\n",
    "#, s=df_cities['users_attendees']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-visiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# total active users unique\n",
    "uniqs = set()\n",
    "for city, groups in city2groups.iteritems():\n",
    "    uniqs.update(extract_active_users(groups))\n",
    "print len(uniqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# unique users who've attended an event in city A\n",
    "# unique users who've attended an event in both city A and city B\n",
    "# why does this happen? another place has more expertise?\n",
    "# your own city is missing a meetup on a particular topic?\n",
    "# we'll restrict ourselves to users who've attended events in\n",
    "# both cities.\n",
    "# other factors: geographic distance, transport\n",
    "# other caveats: triangle effect (lives in London, visits Oxf and Camb)\n",
    "\n",
    "N = len(all_cities)\n",
    "matrix_covisitors = np.zeros([N, N], np.float64)\n",
    "\n",
    "covis_counts = collections.Counter()  # citya, cityb -> num co-visitors\n",
    "vis_counts = {}  # city -> num co-visitors\n",
    "\n",
    "for indx_i, city_i in enumerate(all_cities):\n",
    "    users_i = frozenset(extract_active_users(city2groups[city_i]))\n",
    "    vis_counts[city_i] = float(len(users_i))\n",
    "    \n",
    "    for indx_j, city_j in enumerate(all_cities):\n",
    "        if indx_i == indx_j:\n",
    "            continue\n",
    "        users_j = frozenset(extract_active_users(city2groups[city_j]))\n",
    "        num_covisitors = float(len(users_i & users_j))\n",
    "        \n",
    "        matrix_covisitors[indx_i,indx_j] = num_covisitors\n",
    "        if indx_i < indx_j:\n",
    "            covis_counts[(city_i,city_j)] = num_covisitors\n",
    "\n",
    "assert np.all(matrix_covisitors == matrix_covisitors.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for (city_i, city_j), c in covis_counts.most_common(8):\n",
    "    print c, city_i, \" /// \", city_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "#fig.set_size_inches(20, 10)\n",
    "\n",
    "#\n",
    "# Compute city-to-city magnitudes\n",
    "pairs = []\n",
    "mags_covisitors = np.zeros([len(covis_counts)], np.float64)  # abs num co-visitors\n",
    "mags_ratios = np.zeros([len(covis_counts)], np.float64)      # ratio s_ij / m_i * m_j\n",
    "for indx, (pair, num_covisits) in enumerate(covis_counts.iteritems()):\n",
    "    cA, cB = pair\n",
    "    pairs.append(pair)\n",
    "    mags_covisitors[indx] = num_covisits\n",
    "    mags_ratios[indx] = num_covisits / (vis_counts[cA] * vis_counts[cB])\n",
    "\n",
    "# alternatives: Gravity coefficient; Jaccard similarity?\n",
    "\n",
    "#\n",
    "# Gen params\n",
    "color = 'darkblue'\n",
    "percentile = 70\n",
    "    \n",
    "#\n",
    "# General basemap params\n",
    "params = {'llcrnrlon': -7.372070, 'llcrnrlat': 49.507377, 'urcrnrlon': 2.5, 'urcrnrlat': 56.7}  # all UK\n",
    "#params = {'llcrnrlon': -5.972070, 'llcrnrlat': 49.807377, 'urcrnrlon': 2.0, 'urcrnrlat': 54.7}  # eng&wal\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "# Draw maps\n",
    "#\n",
    "for ax, mags in zip([ax1, ax2], [mags_covisitors, mags_ratios]):\n",
    "    basem = default_basemap(ax=ax, draw_fua=False, draw_london=False, **params)\n",
    "    \n",
    "    # viz\n",
    "    mag_viz_min = np.percentile(mags, percentile)\n",
    "    \n",
    "    max_lw = 2.5\n",
    "    norm_lw = mpl.colors.Normalize(-max(mags)/2, max(mags)*1.2)\n",
    "\n",
    "    norm_color = mpl.colors.Normalize(-max(mags)/1.8, max(mags))\n",
    "    for (cA, cB), mag in zip(pairs, mags.flat):\n",
    "        if mag <= mag_viz_min:\n",
    "            continue\n",
    "        basem.drawgreatcircle(cA.lon, cA.lat, cB.lon, cB.lat, c=color,\n",
    "                              alpha=norm_color(mag),  #alpha=0.6, \n",
    "                              lw=norm_lw(mag)*max_lw)\n",
    "    \n",
    "    x, y = basem(df_cities['lon'].values, df_cities['lat'].values)\n",
    "    sc = basem.scatter(x, y, s=18, marker='o', linewidths=0, c=color)  # 'RdYlBu' 'winter'\n",
    "    \n",
    "    for indx, row in df_cities.iterrows():\n",
    "        x, y = basem(row.ix['lon'], row.ix['lat'])\n",
    "        ax.text(x, y, shortname(row['name']))\n",
    "    \n",
    "    add_frame(ax, ec='k', lw=6.0)\n",
    "\n",
    "ax1.set_title('City-City Co-visitor Network')\n",
    "ax2.set_title('City-City Co-visitor Network (Normalised)')\n",
    "\n",
    "#fig.tight_layout()\n",
    "fig.set_size_inches(13, 7.0)\n",
    "fig.savefig('out/uk_covisitors.pdf', bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "# taking top k-percentile here means that absences of links are also significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "g = nx.Graph()\n",
    "for (cA, cB), mag in zip(pairs, np.array(mags_ratios)):\n",
    "    g.add_edge(cA, cB, weight=mag)\n",
    "    \n",
    "node2cent = nx.katz_centrality(g)\n",
    "for node, cent in sorted(node2cent.items(), key=lambda item: item[1], reverse=True):\n",
    "    print cent, node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build clustering of related groups via an LSA (hidden) topic model trained on keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topic model will be trained on all 'active' groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# recall all_city2groups from earlier. this gives us the active groups in the\n",
    "# UK, including those will filtered out.\n",
    "\n",
    "groupname2doc = {}\n",
    "seq_groups = sum(all_city2groups.values(), [])\n",
    "corpus = []\n",
    "for group in seq_groups:\n",
    "    doc = []\n",
    "    for topic in group['topics']:\n",
    "        txt = topic['urlkey']\n",
    "        assert ' ' not in txt\n",
    "        doc.append(txt)\n",
    "    corpus.append(doc)\n",
    "    groupname2doc[group['name']] = doc\n",
    "assert len(seq_groups) == len(corpus)\n",
    "\n",
    "print \"num groups\", len(seq_groups)\n",
    "print \"num unique topics\", len(frozenset(sum(corpus, [])))\n",
    "corpus = [','.join(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenizer(s):\n",
    "    return s.split(',')\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenizer)  #max_df=0.5,stop_words='english'#~\n",
    "#vectorizer = CountVectorizer(tokenizer=tokenizer)  #max_df=0.5,stop_words='english'#~\n",
    "X_tfidf = vectorizer.fit_transform(corpus)\n",
    "\n",
    "X_tfidf.shape\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "print X_tfidf.shape\n",
    "print \"num features \", len(feature_names)\n",
    "print \"num groups   \", len(seq_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply LSA -- Build latent topic space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsa = TruncatedSVD(n_components=110)  # default=2\n",
    "lsa = lsa.fit(X_tfidf)\n",
    "\n",
    "print sum(lsa.explained_variance_ratio_ )\n",
    "print lsa.explained_variance_ratio_[:10], '...?'\n",
    "\n",
    "X_lsa = lsa.transform(X_tfidf)\n",
    "X_lsa = Normalizer(copy=False).fit_transform(X_lsa)\n",
    "\n",
    "print X_lsa.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group clustering -- number of clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_clusters = np.arange(1,20)\n",
    "cluster_internal_err = np.zeros([num_clusters.size], np.float64)\n",
    "\n",
    "for indx in xrange(num_clusters.size):\n",
    "    k = num_clusters[indx]\n",
    "    \n",
    "    # compute k means and errs\n",
    "    kmeans = KMeans(n_clusters=k, random_state=733)  #~ not cosine!\n",
    "    labels = kmeans.fit_predict(X_lsa)\n",
    "    \n",
    "    # compute errs\n",
    "    clust_errs = np.zeros([k])  # err in each cluster\n",
    "    for clust_id in xrange(k):\n",
    "        rows = X_lsa[labels==clust_id,:]\n",
    "        dist_mat = sklearn.metrics.pairwise.cosine_distances(rows)  #~ cosine!\n",
    "        err = np.sum(np.power(dist_mat, 2)/2.0) / rows.shape[0]  # mean squared error\n",
    "        clust_errs[clust_id] = err\n",
    "    cluster_internal_err[indx] = np.mean(clust_errs)\n",
    "\n",
    "plt.plot(num_clusters, cluster_internal_err)\n",
    "plt.ylabel('Average Within-Cluster MSE')\n",
    "plt.xlabel('k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chosen clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 7\n",
    "kmeans = KMeans(n_clusters=k, random_state=733)  #~ not cosine\n",
    "labels = kmeans.fit_predict(X_lsa)\n",
    "\n",
    "print labels.shape\n",
    "print labels.min(), labels.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = sklearn.decomposition.PCA(n_components=3)\n",
    "pca = pca.fit(X_lsa)\n",
    "X_pca = pca.transform(X_lsa)\n",
    "print pca.explained_variance_ratio_\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(figsize=[16, 4.8])\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax1.scatter(X_pca[:,0], X_pca[:,1], X_pca[:,2], c=labels, s=4.5, lw=0.1)\n",
    "sc = ax2.scatter(X_pca[:,0], X_pca[:,1], c=labels, s=8, lw=0.2, alpha=0.8)\n",
    "\n",
    "ax1.elev = 30    # default 30\n",
    "ax1.azim = -60   # default -60\n",
    "\n",
    "plt.colorbar(sc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster2names = collections.defaultdict(lambda: [])\n",
    "    # map cluster ID to list of group names belonging to cluster\n",
    "    \n",
    "cluster2rowindexes = collections.defaultdict(lambda: [])\n",
    "    # map cluster ID to list of row indexes in the X_* matrices\n",
    "\n",
    "for row_indx, (cluster_id, group) in enumerate(zip(labels, seq_groups)):\n",
    "    cluster2rowindexes[cluster_id].append(row_indx)\n",
    "    cluster2names[cluster_id].append(group['name'])\n",
    "\n",
    "#\n",
    "# finally, build assignment of groupid to clusterid\n",
    "groupid2clusterid = dict(zip([group['id'] for group in seq_groups], labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def top_terms(row_indx, k=3):\n",
    "    # return the top k terms and their weights\n",
    "    vec = X_tfidf[row_indx,:]\n",
    "    items = zip(feature_names, vec.toarray().flat)\n",
    "    items.sort(key=lambda item: item[1], reverse=True)\n",
    "    return items[:k]\n",
    "\n",
    "cluster2clustername = {0: 'Web & Mobile', 1: 'Mixed/Unclassified',\n",
    "                       2: 'DevOps, NoSQL, & Cloud', 3: 'Startups, Networking, & Soft Skills',\n",
    "                       4: 'Makers', 5: 'Agile & Project Management', 6: 'Big Data & Data Science'}\n",
    "\n",
    "print \"latent topics  \", lsa.n_components\n",
    "print \"clusters       \", kmeans.n_clusters\n",
    "\n",
    "print \"\\n==\"\n",
    "print \"==\"\n",
    "print \"==\\n\"\n",
    "\n",
    "for clust_id, grp_names in cluster2names.iteritems():\n",
    "    print clust_id, len(grp_names)\n",
    "\n",
    "print \"\\n==\"\n",
    "print \"==\"\n",
    "print \"==\\n\"\n",
    "\n",
    "\"\"\"\n",
    "for clust_id, grp_names in cluster2names.iteritems():\n",
    "    row_indexes = cluster2rowindexes[clust_id]\n",
    "    \n",
    "    print \"\\n\"\n",
    "    print \"=\"*80\n",
    "    print 'CLUSTER %s    (%s groups)' % (clust_id, len(grp_names))\n",
    "    print \"Cluster name:\", cluster2clustername[clust_id]\n",
    "    print\n",
    "    \n",
    "    for name, row_indx in zip(grp_names, row_indexes):\n",
    "        assert name == seq_groups[row_indx]['name']\n",
    "        print name.upper(), \"     [%d]\" % groupid2clusterid[seq_groups[row_indx]['id']]\n",
    "        print \"   \" + ', '.join(\"%s (%.2f)\" % (term, tfidf) for term, tfidf in top_terms(row_indx, k=2))\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build profile of each city according to group type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cit pre-processing, feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_cities = city2groups.keys()\n",
    "    # NB not necessairly same ordering as mags_ratios\n",
    "\n",
    "corpus = []\n",
    "    # each doc is a city\n",
    "\n",
    "for city in seq_cities:\n",
    "    groups = city2groups[city]\n",
    "    clust_ids = [groupid2clusterid[group['id']] for group in groups]\n",
    "    doc = ','.join(map(str, clust_ids))\n",
    "    corpus.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenizer(s):\n",
    "    return s.split(',')\n",
    "\n",
    "#vectorizer = TfidfVectorizer(tokenizer=tokenizer)  #max_df=0.5,stop_words='english'#~\n",
    "vectorizer = CountVectorizer(tokenizer=tokenizer)  #max_df=0.5,stop_words='english'#~\n",
    "\n",
    "X_count = vectorizer.fit_transform(corpus).astype(np.float64)\n",
    "X_freq = X_count.copy()\n",
    "\n",
    "for row in xrange(X_freq.shape[0]):\n",
    "    X_freq[row,:] = X_freq[row,:] / X_freq[row,:].sum()\n",
    "\n",
    "print X_freq.shape\n",
    "for indx in range(len(seq_cities))[:5]:\n",
    "    print seq_cities[indx]\n",
    "    print X_freq.toarray()[indx,:]\n",
    "print \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity between cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cities_cosine_dist = sklearn.metrics.pairwise.cosine_distances(X_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction via PCA (and MDS cosine similarity for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = sklearn.decomposition.PCA(n_components=3)  # PCA euclidian\n",
    "X_pca = pca.fit_transform(X_freq.toarray())\n",
    "print pca.explained_variance_ratio_\n",
    "comp1 = X_pca[:,0]\n",
    "comp2 =  X_pca[:,1]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=[8,4])\n",
    "sc = plt.scatter(comp1, comp2, linewidths=0)\n",
    "\n",
    "plt.xlabel('PCA comp 1')\n",
    "plt.ylabel('PCA comp 2')\n",
    "plt.title('UK cities')\n",
    "\n",
    "for x, y, name in zip(comp1, comp2, seq_cities):\n",
    "    plt.annotate(shortname(name), xy=[x, y], fontsize=8, textcoords='offset points', xytext=[2, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# seq_cities = city2groups.keys()\n",
    "# co-ordered with...\n",
    "#   cities_cosine_dist\n",
    "#   X_freq\n",
    "#   X_count\n",
    "\n",
    "from sklearn.manifold import MDS\n",
    "dist = sklearn.metrics.pairwise.cosine_distances(X_count.toarray())\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=2)\n",
    "X_r = mds.fit_transform(dist)  # shape (n_components, n_samples)\n",
    "comp1 = X_r[:,0]\n",
    "comp2 =  X_r[:,1]\n",
    "\n",
    "#print ',   '.join(map(str, seq_cities))\n",
    "#print X_count.toarray()\n",
    "#for pair in zip(dist[0,:], map(str, seq_cities)):\n",
    "#    print pair\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=[10,6])\n",
    "\n",
    "# add city names\n",
    "for x, y, city_obj in zip(comp1, comp2, seq_cities):\n",
    "    ax.annotate(shortname(city_obj), xy=[x, y], fontsize=10,\n",
    "                textcoords='offset points', xytext=[4, 0])\n",
    "\n",
    "# num groups per city\n",
    "seq_num_groups = [float(len(city2groups[city])) for city in seq_cities]\n",
    "norm = mpl.colors.LogNorm(vmin=5, vmax=max(seq_num_groups))\n",
    "cmap = mpl.cm.ScalarMappable(norm=norm, cmap=palettable.colorbrewer.sequential.Blues_3.get_mpl_colormap())\n",
    "seq_cols = map(cmap.to_rgba, seq_num_groups)\n",
    "\n",
    "# plot components\n",
    "sc = ax.scatter(comp1, comp2, s=35, facecolor=seq_cols, edgecolor='0.3', linewidths=1)\n",
    "\n",
    "# cbar\n",
    "cmap.set_array(seq_num_groups)\n",
    "cbar = fig.colorbar(cmap, shrink=0.8)\n",
    "cbar.set_label('Number of Groups')\n",
    "cbar.formatter = mpl.ticker.FuncFormatter(lambda a, b: \"%d\" % a)\n",
    "cbar.update_ticks()\n",
    "\n",
    "# aesthetics\n",
    "ax.set_xlabel('Comp 1')\n",
    "ax.set_ylabel('Comp 2')\n",
    "ax.set_title('Similarity Map of Tech Communities')\n",
    "\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.yaxis.set_visible(False)\n",
    "#ax.set_xlim(left=-.33, right=0.25);\n",
    "#ax.set_ylim(bottom=-0.12, top=0.1);\n",
    "ax.set_axis_bgcolor('w')\n",
    "add_frame(ax, fc='none', ec='#888888', lw=5)\n",
    "\n",
    "fig.savefig('out/cities_mds_similarity.pdf', bbox_inches='tight', pad_inches=0)\n",
    "# distance between cities indicates similarity. closer more similar.\n",
    "# angle has no meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cities by meetup group types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#vectorizer\n",
    "#X_freq\n",
    "#X_counts\n",
    "\n",
    "df_type_counts = pandas.DataFrame(X_count.toarray(),\n",
    "    columns=[cluster2clustername[int(clust_id)] for clust_id in vectorizer.get_feature_names()],\n",
    "    index=[shortname(c) for c in seq_cities])\n",
    "\n",
    "print df_type_counts\n",
    "\n",
    "print vectorizer.get_feature_names()  # cluster IDs (ints as strings)\n",
    "df_type_freqs = pandas.DataFrame(X_freq.toarray(),\n",
    "    columns=[cluster2clustername[int(clust_id)] for clust_id in vectorizer.get_feature_names()],\n",
    "    index=[shortname(c) for c in seq_cities])\n",
    "\n",
    "# sort\n",
    "row_indxs = df_type_counts.sum(axis=1).argsort()[::-1]\n",
    "df_type_counts = df_type_counts.iloc[row_indxs,:]\n",
    "df_type_freqs = df_type_freqs.iloc[row_indxs,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, [ax1, ax2, ax3] = plt.subplots(1, 3, figsize=[12, 2.5])\n",
    "\n",
    "# count of number of groups\n",
    "df_counts = df_type_counts.sum(axis=1)\n",
    "df_counts.plot(kind='bar', ax=ax1, color='k')\n",
    "ax1.set_ylim(ymax=100)\n",
    "ax1.text(0.1, 104, int(df_counts.ix['London']), rotation=90, ha='center', va='bottom')\n",
    "#ax1.arrow(0.1, 0, 0.1, 64, int(df_counts.ix['London']), rotation=90, ha='center', va='bottom')\n",
    "ax1.set_title(\"Number of Groups\")\n",
    "\n",
    "# proprtions in middle\n",
    "(df_type_freqs*100).plot(kind='bar', stacked=True, ax=ax2)\n",
    "ax2.set_title('Community Composition')\n",
    "ax2.set_ylim(ymax=100);\n",
    "ax2.set_ylabel('Groups (%)')\n",
    "\n",
    "# move legend to RHS\n",
    "handles, labels = ax2.get_legend_handles_labels()\n",
    "ax3.legend(handles=handles, fontsize='medium', frameon=False, loc='center left')\n",
    "ax3.axis('off')\n",
    "ax2.legend_.remove()\n",
    "\n",
    "#fig.tight_layout()\n",
    "fig.subplots_adjust(wspace=0.35)\n",
    "\n",
    "fig.savefig('out/uk_num_groups_bar_charts.pdf', bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The map of events, by group type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_ids = sorted(set(cluster2rowindexes.keys()))\n",
    "\n",
    "norm = mpl.colors.Normalize(vmin=min(cluster_ids), vmax=max(cluster_ids))\n",
    "cmap = mpl.cm.ScalarMappable(norm=norm, cmap='Set1')\n",
    "clusterid2color = {cid: cmap.to_rgba(cid) for cid in cluster_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_lon = []\n",
    "seq_lat = []\n",
    "seq_col = []\n",
    "for group in seq_groups:\n",
    "    for event in group['events_in_window']:\n",
    "        if 'venue' not in event:\n",
    "            continue\n",
    "        venue = event['venue']\n",
    "        seq_lon.append(venue['lon'])\n",
    "        seq_lat.append(venue['lat'])\n",
    "        clust_id = groupid2clusterid[group['id']]\n",
    "        seq_col.append(clusterid2color[clust_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes, mark_inset \n",
    "\n",
    "#\n",
    "# Basemap\n",
    "fig = plt.figure(figsize=[18, 12])\n",
    "#gs = mpl.gridspec.GridSpec(2, 3, wspace=0.02, hspace=0.01)\n",
    "#ax1 = plt.subplot(gs[:,:-1])\n",
    "#ax2 = plt.subplot(gs[0,-1])\n",
    "#ax3 = plt.subplot(gs[1,-1])\n",
    "gs = mpl.gridspec.GridSpec(1, 3, wspace=0.10, hspace=0.01, width_ratios=[5, 6, 10])\n",
    "ax1 = plt.subplot(gs[0,0])\n",
    "ax2 = plt.subplot(gs[0,1])\n",
    "ax3 = plt.subplot(gs[0,2])\n",
    "\n",
    "viewports = [\n",
    "    {'llcrnrlon': -11.372070, 'llcrnrlat': 48.507377, 'urcrnrlon': 2.5, 'urcrnrlat': 59.7},  # UK\n",
    "    {'llcrnrlon': -5.972070, 'llcrnrlat': 49.807377, 'urcrnrlon': 2.0, 'urcrnrlat': 54.7},  # eng&wal\n",
    "    {'llcrnrlon': -0.206744, 'llcrnrlat': 51.48, 'urcrnrlon': -0.01, 'urcrnrlat': 51.560},  # Central London\n",
    "    ]\n",
    "\n",
    "for ax, view, s, alpha in zip([ax1, ax2, ax3], viewports, [1.5, 1.5, 3.5], [0.2, 0.2, 0.5]):\n",
    "    basem = default_basemap(ax=ax, draw_fua=True, draw_london=True,**view)\n",
    "\n",
    "    x, y = basem(seq_lon, seq_lat)\n",
    "    \n",
    "    basem.scatter(x, y, s=s, marker='o', linewidths=0, c=seq_col, alpha=alpha)\n",
    "\n",
    "ax1.set_title('UK')\n",
    "ax2.set_title('England & Wales')\n",
    "ax3.set_title('London')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity vs geography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inputs (computed earlier)...\n",
    "#\n",
    "# seq_cities = city2groups.keys()\n",
    "# co-ordered with...\n",
    "#   cities_cosine_dist\n",
    "#   X_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairwise_covis_ratios = {}\n",
    "pairwise_cosine_dists = {}\n",
    "for i, cA in enumerate(seq_cities):\n",
    "    for j, cB in enumerate(seq_cities):\n",
    "        if i < j:\n",
    "            pair = (cA, cB)\n",
    "            ratio = covis_counts[pair] / float(vis_counts[cA] * vis_counts[cB])\n",
    "            pairwise_covis_ratios[pair] = ratio\n",
    "            pairwise_cosine_dists[pair] = cities_cosine_dist[i,j]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_dists = []\n",
    "seq_ratios = []\n",
    "for pair in pairwise_covis_ratios.keys():\n",
    "    seq_ratios.append(pairwise_covis_ratios[pair])\n",
    "    seq_dists.append(pairwise_cosine_dists[pair])\n",
    "seq_ratios = np.array(seq_ratios)\n",
    "seq_ratios = seq_ratios / seq_ratios.max()\n",
    "print len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.scatter(seq_dists, seq_ratios, marker='x', s=1.0)\n",
    "ax.set_xlabel('cosine distance')\n",
    "ax.set_ylabel('gravity ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Comparison with naive approach -- PCA on city tag counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combined interactive map\n",
    "# mpld3.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
